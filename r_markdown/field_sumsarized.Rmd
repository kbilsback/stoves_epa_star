---
title: "field_sumsarized"
author: "ricardo_piedrahita"
date: "8/10/2017"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r library, message=FALSE, warning=FALSE, echo=FALSE}
  source('../r_scripts/load_2.R')

```

```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.path='figures/', warning=FALSE, message=FALSE, cache=FALSE)
```

```{r functions}
  source("../r_scripts/functions.R")
  source("../r_scripts/plots.R")
  source("../r_scripts/filter_sum_data.R")
```

# Load data

* Time stamps are fixed in the data importing step

```{r load_data}
  field_sumsarized_events <- readRDS("../r_files/field_sumsarized_events.RDS") #Data analyzed by Geocene, directly giving events
  field_sumsarized_durations <- readRDS("../r_files/field_sumsarized_durations.RDS") #CSVs used by Geocene to get events.  These are needed so that we know the sampling time duration for average usage stats.
  field_sumsarized_durations_csvs <- readRDS("../r_files/field_sumsarized_durations_csvs.RDS") #CSVs used by Geocene to get events.  These are needed so that we know the sampling time duration for average usage stats.
  field_sumsarized_timeseries <- readRDS("../r_files/field_sumsarized_timeseries.RDS") #Import data that was analyzed with the web-based sumsarizer tool by RP, not by Geocene.
  field_sumsarized_timeseries_india <- readRDS("../r_files/field_sumsarized_timeseries_India_recovered.RDS") #Import data that was analyzed with the web-based sumsarizer tool by RP, not by Geocene.
  field_sumsarized_timeseries<-rbind(field_sumsarized_timeseries,field_sumsarized_timeseries_india)
```

* metadata

```{r load_meta}
  field_test_times <- readRDS("../r_files/field_test_times.RDS")
  field_samples <- readRDS("../r_files/field_samples.RDS")
  field_temp_meta <- readRDS("../r_files/field_temp_meta.RDS")
  field_notes <- readRDS("../r_files/field_notes.RDS")
  honduras_behavior <-readRDS("../r_files/honduras_behavior.RDS") #Import self-reported data from Honduras.
```

* Define variables for coding stoves and for filtering out data.  Also set some constants for the stove event analysis

```{r define_variables}
  stove_codes <- data.frame(stove = as.factor(c("Metal jiko",
                "Traditional ceramic","Metal jiko ceramic","Uga",
                  "Traditional ceramic metal clad","Kerosene","Tire Rim",
                  "Traditional chimney","KNG","CLH","CLC","IBP","CLH1","Other",
                  "PB-OK","PB-IK", "PB-IK-E","LPG","TSF","PB-OK Biomass mobile",
                  "RS-IK","chimney","traditional","chimney2","nochimney","electric","propane","traditional2")),
            stove_descriptions = as.factor(c("Charcoal stove", "Charcoal stove", "Charcoal stove",
                  "Charcoal stove","Charcoal stove","Kerosene", "Tire rim","Chimney",
                  "Kang","Coal chimney stove","Coal mobile without chimney",
                  "Biomass pellet with chimney", "Coal chimney stove","Other",
                  "Local traditional stove", "Local traditional stove", "Local traditional stove", "LPG", "Local traditional stove", "Rocket", 
                  "Rocket","Chimney stove","Local traditional stove","Chimney stove","Local traditional stove","Electric",
                  "LPG","Local traditional stove")))
  
  cooking_group <- 60 # x minute window for grouping events together.
  cooking_duration_minimum <- 5   #Minutes
  cooking_duration_maximum <- 1440 #Minutes
  logging_duration_minimum <- 1 #days.  duration must be this long to be analyzed.
  
  
  # Remove data from files selected as bad (they match the following strings: 0_0_3_P54637_IndiaDMYminave
  bad_files <- paste(c("0_0_3_P83601_IndiaDMYminave","|0_0_3_P46673_Indiaminaves","|0_0_3_P54588_Indiaminaves"
                 ,"|0_0_3_P46673_Indiaminaves","|0_0_0_P57934_Indiaminaves","|0_0_0_P83591_IndiaDMYminaves"
                 ,"|0_0_0_P83601_Indiaminaves","|0_0_2_P57934_Indiaminaves","|0_0_2_P83591_Indiaminaves"
                 ,"|0_0_3_P54637_IndiaDMYminaves","|0_0_4_P46673_IndiaDMYminaves","|IN7_P54593_20160419_Indiaminaves" 
                 ,"|IN8_P83591_20160419_Indiaminaves","|IN8_P83591_20160503_IndiaDMYMinAves",
                 "|IN10_P57934_20160427_Indiaminaves"
                 ,"|IN10_P57934_20160503_Indiaminaves","|IN11_P46673_20160420_Indiaminaves",
                 "|IN11_P46673_20160503_IndiaDMYMinAves"
                 ,"|IN12_P54637_20160503_IndiaDMYMinAves","|INXX_P53257_20160430_Indiaminaves",
                 "|INXX|0_0_0_P53257_IndiaMinAves"),collapse="")

```

# Tidy

* add household id information, get metadata from metadata file and from file names, depending on country
```{r parse_add_metadata}

#There are some files that contributed zero cooking events, so we will mark those with events as 'true', and then create entries with the other files as 'false'.
field_sumsarized_events <- dplyr::mutate(field_sumsarized_events,use_flag = TRUE) 


#get files that are not contributing cooking events.
matched <- intersect(field_sumsarized_durations_csvs$filename, field_sumsarized_events$filename)
all <-  union(field_sumsarized_durations_csvs$filename, field_sumsarized_events$filename)
non.matched <- all[!all %in% matched]

   # #Initialize the empty_events date frame.
empty_events <- data.frame(filename=character(),
                      start_time=as.POSIXct(character()),
                      duration_minutes=as.numeric(),
                      hh_id=factor(),
                      stove=factor(),
                      logger_id=factor(), 
                      field_site=factor(),  
                      stove_use_category=factor(),
                      start_date = as.POSIXct(character()), 
                      end_date = as.POSIXct(character()),
                      notes=character(),
                      event_num=as.numeric(),
                      use_flag = as.logical())

for (i in 1:length(non.matched)) {
      #File names aren't identical so look for partial matches.
      #Grab data from file i, and keep only the entries that are marked as cooking
      datarow <- field_sumsarized_durations_csvs[grepl(non.matched[i],field_sumsarized_durations_csvs$filename),]
      
      empty_events <- rbind(empty_events,data.frame(filename=datarow$filename,
      event_num = i*100.1, start_time = datarow$file_start_date[1],
      duration_minutes=0, max_temp=NA,
min_temp=NA,stdev_temp=NA, use_flag = FALSE))
}

field_sumsarized_events <- rbind(field_sumsarized_events,empty_events)

#Use the same filtering on both the data set prepared by Geocene that gave events, and the data output by Sumsarizer that gives a time series.
field_sumsarized_events_all_temp<-filter_sum_data(field_sumsarized_events,field_temp_meta,1,cooking_group)


field_sumsarized_events_all <-dplyr::left_join(field_sumsarized_events_all_temp,
                                    dplyr::select(field_sumsarized_durations, filename,
                                    logging_duration_days,file_start_date,file_end_date),  by = "filename") %>%
                              dplyr::left_join(dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                              dplyr::group_by(event_num) %>% #There are some duplicate cooking event numbers because during the join between the meta data and the data set, it was not clear which sum was used to generate the cooking event...but it does not matter, because the stove and hhid were the same.  This was the case for Uganda data that had the hhid in the filename and was used for matching rather than the sum id.
                              dplyr::filter(row_number() == 1) %>%
                              dplyr::ungroup() %>% ##If data has an end time, keep only data between the start and end time
                              dplyr::filter(((start_time<=end_date & start_time>=start_date) | is.na(end_date))) %>%
                              dplyr::mutate(start_date = if_else(is.na(start_date),file_start_date,
                                    if_else(file_start_date<=start_date,start_date, file_start_date))) %>%
                                    #if there is no start_date in the log sheet, use the logging start date.  Otherwise use the log sheet one, unless it is incorrect and shows up before the file_start_date, then use the file_start_date
                              dplyr::mutate(end_date = if_else(is.na(end_date),file_end_date,
                                    if_else(file_end_date>=end_date,end_date, file_end_date))) %>%
                              dplyr::mutate(units = "degrees Celsius") %>% #Define units as degrees C
                              # dplyr::filter(duration_minutes>cooking_duration_minimum | use_flag==FALSE) %>%
                              # dplyr::filter(duration_minutes<cooking_duration_maximum) %>%
                              dplyr::group_by(filename) %>%
                              dplyr::mutate(events_per_file = n()) %>%
                              dplyr::mutate(events_per_day = events_per_file/logging_duration_days) %>%
                              dplyr::select(filename,start_time,duration_minutes,hh_id,stove,
                                           logger_id,field_site,stove_use_category,stove_descriptions,
                                           stove,logging_duration_days,start_date,
                                           end_date,units,notes,events_per_day,use_flag)

  
#Import thermocouple data time series output from Sumsarizer.
field_sumsarized_timeseries_all <-filter_sum_data(field_sumsarized_timeseries,field_temp_meta,0,0)
field_sumsarized_timeseries_all <- dplyr::filter(field_sumsarized_timeseries_all,((datetime<end_date & datetime>start_date) | is.na(end_date))) #If there is a start and end time available for the monitoring period, use it to keep the data from the file.  Disregard this if the end_date is NA, since it means we don't have a fixed known end date.  In this case we assume the end of the file is the end of the monitoring preiod and keep all the data from the given file.  Provide the start and end date in the event-building loop below.


```

  
* Import thermocouple sumsarizer data
* Form cooking events from raw sumsarizer output don't need it for the event data generated by Danny Wilson
* Group distinct cooking events together.  If they are within cooking_group minutes of each other.
* Only need this for data that went through the web-based sumsarizer, where events are not made automatically.

```{r group_cooking_events}

# # start counting up for cooking events.  Add columns for start and stop times of cooking events, and keep the hhid, loggerid, stove type, field site.  Keep only cooking event time points.  Need a case for start (if i = 1), if data point is within 30 minutes of the last one, if data point is greater than 30 minutes from the last one.

# #Initialize the cooking_events frame.
 cooking_events_timeseries <- data.frame(start_time=as.POSIXct(character()),
                    end_time=as.POSIXct(character()), field_site=factor(),  hh_id=factor(),
                    logger_id=factor(),stove=factor(), logging_duration_days=as.numeric(),
                    start_date = as.POSIXct(character()), end_date = as.POSIXct(character()),
                    file_indices = as.numeric(),events_per_file =as.numeric(),
                    filename=character(),notes=character(),stove_use_category=factor(),use_flag = as.logical())

 #for each SUM data file
 for (i in unique(field_sumsarized_timeseries_all$file_indices)) {
   #Grab data from file i, and keep only the entries that are marked as cooking
   temp <- dplyr::filter(field_sumsarized_timeseries_all,file_indices == i) %>%
     dplyr::filter(state == TRUE) %>% dplyr::filter(!duplicated(datetime)) 
   #if any cooking time is found, make an event from it.
   
         #If date is NA, give it a good one.
      if(is.na(as.POSIXct(temp$end_date[1]))) { 
             start_date_temp = as.POSIXct(min(temp$datetime))
             end_date_temp = as.POSIXct(max(temp$datetime))
       } else if (temp$end_date[1] == as.POSIXct('2017-01-01')) { #If end date is 2017-01-01, this was used in the log sheet as code that we did not know the end date.  use the values from the files in this case.
             start_date_temp = as.POSIXct(min(temp$datetime))
             end_date_temp = as.POSIXct(max(temp$datetime))
       } else { #If not NA and not 2017-01-01, must have been found in the meta data, use it.
             start_date_temp = as.POSIXct(min(temp$datetime))
             end_date_temp = as.POSIXct(max(temp$datetime))
       }    
   
   if (dim(temp)[1]>1) {
     
      difftime <- as.numeric(diff(temp$datetime)) #Time between identified cooking samples, in minutes
     
      breakstart <- c(0,which((difftime>cooking_group) == TRUE))+1 #Start of a cooking event
      breakend <- c(which((difftime>cooking_group) == TRUE),
            if (tail(temp$state,n=1) == TRUE){dim(temp)[1]}) #End of cooking event. 
        #Tail part is in case the time series ends while still cooking...need to account for th

      #Add cooking events to the cooking_events_timeseries data frame.
      cooking_events_timeseries <- rbind(cooking_events_timeseries,
                  data.frame(start_time= as.POSIXct(temp$datetime[breakstart]),
                  end_time=as.POSIXct(temp$datetime[breakend]), field_site=as.factor(temp$field_site[breakstart]),
                  hh_id=as.factor(temp$hh_id[breakstart]),
                  logger_id=factor(temp$logger_id[breakstart]),stove=factor(temp$stove[breakstart]),
                  logging_duration_days = as.numeric(temp$logging_duration_days[1]),
                  end_date = end_date_temp,
                  start_date = start_date_temp,
                  file_indices = temp$file_indices[breakstart], 
                  events_per_file = length(breakstart)*breakstart/breakstart,
                  filename = temp$filename[breakstart],
                  notes = temp$notes[1],
                  stove_use_category=factor(temp$stove_use_category[breakstart]),
                  use_flag=as.logical(rep(1,length(breakstart)[1]))))
    } else{
         #If no cooking events are found, still create an entry, though with the use flag as FALSE, so 
         #that we know that there was data collected and zero events in that period.
       temp <- dplyr::filter(field_sumsarized_timeseries_all,file_indices == i)  #Create new temp here so 
       #we can get info about the sample that does not have cooking events.
       cooking_events_timeseries <- rbind(cooking_events_timeseries,
                  data.frame(start_time=as.POSIXct(temp$datetime[1]),
                  end_time=as.POSIXct(temp$datetime[1]), field_site=as.factor(temp$field_site[1]),
                  hh_id=as.factor(temp$hh_id[1]), 
                  logger_id=factor(temp$logger_id[1]),stove=factor(temp$stove[1]), 
                  logging_duration_days=as.numeric(temp$logging_duration_days[1]),
                  end_date = end_date_temp,
                  start_date = start_date_temp,
                  file_indices = temp$file_indices[1], 
                  events_per_file = 0,
                  filename = temp$filename[1],
                  notes = temp$notes[1],
                  stove_use_category=factor(temp$stove_use_category[breakstart]),
                  use_flag=FALSE))
    }
 }


#Clean up cooking events.  Removing events with a single sample that indicates cooking, more than 30 minutes from other events.
 cooking_events_timeseries <- dplyr::left_join(cooking_events_timeseries,
                                              dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                              dplyr::mutate(units = "degrees Celsius") %>% #Define units as degrees C
                              dplyr::mutate(duration_minutes = as.numeric(difftime(end_time,start_time,units = "mins"))) %>%
   #Filter out events shorter than the threshold, while keeping the entries that have no uses in the deployments.
                              dplyr::filter(duration_minutes>cooking_duration_minimum | use_flag==FALSE) %>%
                              dplyr::mutate(events_per_day = events_per_file/logging_duration_days) %>%
                              dplyr::select(filename,start_time,duration_minutes,hh_id,stove,logger_id,field_site,
                              stove_use_category,stove_descriptions,stove,logging_duration_days,start_date,
                                end_date, units,notes,events_per_day,use_flag)

 #Data summaries
 # simpledatasummary <- summarise_each(field_sumsarized_events_all,funs(n_distinct(.)))
 # 
 # grouped_summary <- cooking_events_timeseries %>% dplyr::group_by(field_site,stove_descriptions) %>%
 #    dplyr::summarize(mean=mean(duration_minutes, na.rm = TRUE), median=median(duration_minutes, na.rm = TRUE),sd=sd(duration_minutes, na.rm = TRUE),n=n())
 # 
 # #Get a single row from each file so we can look at the stats calculated previously.
 #  hh_file_average_summary <- dplyr::filter(cooking_events_timeseries,!duplicated(filename))
```

* Create and save final cooking event data frame.

```{r clean_cooking_events}

#Combine cooking events from the SUMsarized data set by Danny, with the ones from the thermocouples.
#Some cooking events don't have an associated HHID or stove type because they could not be found in the meta data file.  Happens for 2-3 files in India.  Could get the stove type through the file name if needed, but don't want to use it since we don't know which hh it comes from and we have plenty of data. 
  all_cooking_events <- plyr::rbind.fill(field_sumsarized_events_all,cooking_events_timeseries) 
  saveRDS(all_cooking_events, file = "../r_files/all_cooking_events.RDS")

```


* set timezones based on field site. Time zones are not easily discernible and I don't have a way to confirm them.  Do not trust time of day trends until resolved.  China has many starting times between 2-4AM, suggesting an incorrectly set computer.  But how would it have been set, MST?   This is definitely the case for some India files, but those are listed in the notes.

```{r fix_timezones}
# #Honduras time zone is +6.  India is +5.5.  China is +8.  Uganda is +3.
# #Need to shift a few files that were logging in MST to GMT.  Then, all 
# all_cooking_events <- dplyr::mutate(all_cooking_events,datetime = 
#                             case_when(grepl("add 11.5",notes), ,
#                                   grepl("india",field_site, fixed=TRUE,ignore.case=TRUE),       
#                                 
#                                 
#                                 
#                               )        lubridate::force_tz(start_time,
#                                                                        tzone = "Asia/Calcutta"))
# 
# 
# dplyr::mutate(field_sumsarized_timeseries_all, datetime = 
#                             if_else(grepl("add 11.5",notes),
#                                 datetime+hours(11) + minutes(30),
#                                 if_else(grepl("india",field_site, fixed=TRUE,ignore.case=TRUE),
#                                     datetime+hours(5) + minutes(30) ,datetime))) %>% #If datapoint has a comment with 11.5, add time.  Otherwise, shift the india data to its correct 5.5+GMT timezone.
#   
#     dplyr::mutate(start_time = if_else(grepl("add 11.5",notes),
#                               start_time+hours(6), start_time)) %>% 
#                               dplyr::mutate(start_time = if_else(grepl("add 11.5",notes), 
#                                 datetime+hours(11) + minutes(30),
#                                 if_else(grepl("india",field_site, fixed=TRUE,ignore.case=TRUE),
#                                     datetime+hours(5) + minutes(30) ,datetime))) %>% #If datapoint has a comment with the text 11.5, add that to the timestamp.
# 

```



# Remove data from bad files:
  * merge flags with data
  * Need to test this
```{r create_flags}
  ok_cooking_events <-  dplyr::mutate(all_cooking_events,
                                       qc = if_else(grepl(bad_files,filename,ignore.case=TRUE),"bad","ok")) %>% 
                          dplyr::mutate(qc = if_else(grepl("IN4",hh_id,ignore.case=TRUE),"maybe",qc)) %>%
                          dplyr::filter(grepl("ok",qc)) %>%
                          dplyr::filter(logging_duration_days > logging_duration_minimum) %>%
                          dplyr::filter(duration_minutes < cooking_duration_maximum) %>%
                          dplyr::filter(!is.na(stove_descriptions)) %>%
                          dplyr::mutate(field_site = if_else(grepl("india",field_site),"India",
                                        if_else(grepl("china",field_site),"China",
                                              if_else(grepl("uganda",field_site),"Uganda",
                                                if_else(grepl("honduras",field_site),"Honduras","NA"))))) %>%
                          dplyr::mutate(start_hour = hour(start_time) + minute(start_time)/60) %>%
                          dplyr::mutate(month_year = as.factor(format(start_time,"%b-%y"))) %>%
                          dplyr::mutate(month_year = factor(month_year, 
                                                            levels = c("Jan-15", "Feb-15", "Mar-15", "Apr-15", 
                                                                "May-15", "Jun-15", "Jul-15", "Aug-15",
                                                                "Sep-15", "Oct-15", "Nov-15", "Dec-15" ,
                                                                "Jan-16", "Feb-16", "Mar-16", "Apr-16", 
                                                                "May-16", "Jun-16", "Jul-16", "Aug-16", 
                                                                "Sep-16", "Oct-16", "Nov-16", "Dec-16"))) %>%
                          dplyr::mutate(day_month_year = as.Date(start_time)) %>%
                          dplyr::mutate(week_year = format(start_time,"%V-%y")) %>%
                          dplyr::mutate(day_of_week = as.factor(weekdays(start_time))) %>%
                          dplyr::mutate(day_of_week = factor(day_of_week, 
                                     levels = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))) %>%
                          dplyr::arrange(desc(end_date)) %>%  #Sort so we toss the earlier end dates when deleting distinct values.
                          dplyr::distinct(start_time,duration_minutes,hh_id,stove_descriptions, .keep_all = TRUE)  #There are some cases that have duplicated events due to overlapping files.  (ie. file started on1/1 and was downloaded on 1/5, but then redeployed without deleting, and redownloaed on 1/9.  So duplicated events from 1/1 to 1/5.  )
ok_cooking_events$start_date <- floor_date(ok_cooking_events$start_date,"minute")


#Create padded time series of cooking events, so there is an event for every day, rather than gaps.  This facilitates stacking and stove use fraction analyses.
ok_cooking_events_padded <- data.frame(stringsAsFactors = FALSE)
#Replace NA with 0.
#dplyr_if_else   <- function(x) { mutate_all(x, funs(ifelse(is.na(.), 0, .))) }
uniquers <- unique(ok_cooking_events$filename)
for (i in 1:length(unique(ok_cooking_events$filename))) {

    temp <- dplyr::filter(ok_cooking_events,uniquers[i]==filename) %>%
                dplyr::arrange(start_time)
        
    if (dim(temp)[1]>0) {    
        # generate a time sequence with 1 day intervals to fill in
        # missing dates
        all.dates <- data.frame(dates = seq(temp$start_date[1], temp$end_date[1], by="day"),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(day_month_year = as.Date(dates)) %>%
                  dplyr::filter(!day_month_year %in% temp$day_month_year) #Get rid of extra days
        
        # Convert all dates to a data frame. Note that we're putting
        # the new dates into a column called "start_time" just like the
        # original column. This will allow us to merge the data.
        all.dates.frame <- data.frame(list(start_time=all.dates$dates),
                                      list(day_month_year = as.Date(all.dates$dates)),
                                      list(week_year = format(all.dates$dates,"%V-%y")),
                                      list(day_of_week = weekdays(all.dates$dates)),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
                  dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE))
                         

        # Merge the two datasets: the full dates and original data
        merged.data <- merge(all.dates.frame, temp, all=T) %>%
            tidyr::fill(filename,filename,hh_id,stove,logger_id,field_site,
                        stove_descriptions,logging_duration_days,start_date,
                        end_date,units,notes,
                        start_hour,.direction = c("up")) %>%
            tidyr::fill(filename,filename,hh_id,stove,logger_id,field_site,
                        stove_descriptions,logging_duration_days,start_date,
                        end_date,units,notes,
                        start_hour,.direction = c("down"))  %>%
            dplyr::mutate(use_flag = replace(use_flag, is.na(use_flag), FALSE)) 
        
        
        merged.data %>% mutate_if(is.factor, as.character) -> merged.data

        merged.data$use_flag[is.na(merged.data$use_flag)] <- FALSE
        merged.data[is.na(merged.data)] <- 0
        # The above merge set the new observations to NA.
        # To replace those with a 0, we must first find all the rows
        # and then assign 0 to them.
        #merged.data <-dplyr_if_else(merged.data)

        ok_cooking_events_padded <- rbind(ok_cooking_events_padded,merged.data)
    }
}
    #time format was messed up somehow in making the padded set, correct it here.
    ok_cooking_events_padded <- dplyr::mutate(ok_cooking_events_padded,end_date = as.POSIXct(end_date,origin = "1970-1-1", tz = "GMT")) %>%
      dplyr::arrange((start_time)) %>%
      dplyr::arrange(desc(stove_descriptions)) %>%  #Sort so we toss the earlier end dates when deleting distinct values.
      dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
      dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE)) %>%
      dplyr::mutate(qc = "ok") #New qc values default to zero during padding, but they are all already qc-filtered, so reset to TRUE

    #Finish filtering the dataset.  Kept 'bad' and short events to get the full padded dataset.  Now can remove them
    ok_cooking_events <- dplyr::filter(ok_cooking_events,duration_minutes>cooking_duration_minimum | use_flag==FALSE) 



```



# Summarize data

```{r summarize_data}

#Remove data from SUMs that were secondary measurements (this happened in Honduras during piloting).  To do this, remove data from chimney2 - these were duplicate placements.

#Clean up the event data, define units, set up grouping variables to organize stats by deployment.
ok_cooking_events <- 
        dplyr::group_by(ok_cooking_events,hh_id,stove_descriptions) %>%   #group_by is handy.
        dplyr::mutate(events_per_hhid_stove = n()) %>%
        dplyr::mutate(minutes_per_hhid_stove = sum(duration_minutes)) %>%
        dplyr::arrange(hh_id) %>%
        dplyr::filter(!is.na(hh_id)) %>%
        dplyr::filter(!is.na(start_date))

ok_datasummary <- summarise_all(ok_cooking_events,funs(n_distinct(.)))



#Summarize use by household, deployment, and stove for plotting average use.
events_summary <- dplyr::filter(ok_cooking_events,!duplicated(filename)) %>% #Remove duplicate files because we only one to get the logging duration from a single deployment
        dplyr::group_by(hh_id,stove_descriptions) %>%   
        dplyr::mutate(days_logged_per_hhid_stove = sum(logging_duration_days))   %>% 
        dplyr::mutate(events_per_day = events_per_hhid_stove/days_logged_per_hhid_stove) %>%
        dplyr::mutate(minutes_per_day_stove_hhid = minutes_per_hhid_stove/days_logged_per_hhid_stove) %>%
        dplyr::distinct(filename, .keep_all = TRUE) %>%
        dplyr::filter(!is.na(stove_descriptions)) %>%
        dplyr::filter(!duplicated(hh_id) & !duplicated(stove_descriptions))

#Summary of all cooking events.  Group by country and stove type to get the stats on durations of all the cooking events.
 grouped_summary_a <- ok_cooking_events %>% dplyr::group_by(field_site,stove_descriptions) %>%
      dplyr::summarize(mean_event_duration=mean(duration_minutes, na.rm = TRUE),
                       median_event_duration=median(duration_minutes, na.rm = TRUE),
                       sd_event_duration=sd(duration_minutes, na.rm = TRUE),
                       households = length(unique(hh_id))) 

 # This is checked and good.  Some hh's have multiple of the same stove type monitored, those and so it counts those and separate consecutive deployments separately, as it should.
grouped_summary_b <-  dplyr::filter(ok_cooking_events,!duplicated(filename)) %>% #Remove dup filenames to get these stats below without duplication.
      dplyr::group_by(field_site,stove_descriptions) %>%
      dplyr::summarize(days_logged_per_site_by_stove=sum(logging_duration_days, na.rm = TRUE)
                       ,nfiles=n())
        
#Get average of average daily uses by stove type
grouped_summary <- events_summary %>% dplyr::group_by(field_site,stove_descriptions) %>%
      dplyr::summarize(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE)) %>%
      dplyr::left_join(grouped_summary_a,by = c("field_site", "stove_descriptions")) %>%
      dplyr::left_join(grouped_summary_b,by = c("field_site", "stove_descriptions")) 

kable(grouped_summary, digits=2)
openxlsx::write.xlsx(grouped_summary, file = paste0("Summary Statistics by stove ",format(now(),"%d-%b-%y"),".xlsx"),sheetName = "by stove")



#Summarize use by site, household, deployment, and stove for plotting average use.
events_summary_hhid <- dplyr::group_by(ok_cooking_events,hh_id) %>%  
        dplyr::mutate(minutes_cooked_per_hhid = sum(duration_minutes)) %>%
        dplyr::filter(!duplicated(filename)) %>% #Remove duplicate files because we only one to get the logging duration from a single deployment
        dplyr::mutate(days_logged_per_hhid = sum(logging_duration_days))   %>% 
        dplyr::mutate(events_per_day = events_per_hhid_stove/days_logged_per_hhid) %>%
        dplyr::mutate(minutes_per_day_hhid = minutes_cooked_per_hhid/days_logged_per_hhid) %>%
        dplyr::distinct(filename, .keep_all = TRUE) %>%
        dplyr::filter(!is.na(stove_descriptions)) %>%
        dplyr::filter(!duplicated(hh_id)) %>%
        dplyr::group_by(field_site) %>%
        dplyr::summarize(mean_minutes_cooking_perday=mean(minutes_per_day_hhid, na.rm = TRUE),
                       median_minutes_cooking_perday=median(minutes_per_day_hhid, na.rm = TRUE),
                       sd_minutes_cooking_perday=sd(minutes_per_day_hhid, na.rm = TRUE),
                       households = length(unique(hh_id))) 

kable(events_summary_hhid, digits=2)
openxlsx::write.xlsx(events_summary_hhid, file = paste0("Summary Statistics by country ",format(now(),"%d-%b-%y"),".xlsx"),sheetName = "by site")


monthStart <- function(x) {
  x<- floor_date(as.Date(x), "month") 
}
monthEnd <- function(x) {
  x<- floor_date(as.Date(x), "month") + months(1)
}

#Summarize use by household/deployment/stove for plotting average use by month. Could do by week as well.  Groups by filename as well, so if there are two downloads from a hh, it is presented as two different data points, even within the same month.  This is probably not ideal, but this plot is not going to be very interesting any way.  Could fix it by ungrouping, regrouping without the filename, taking the sums of the numerators and denominators, and then taking the ratios.
events_summary_monthly <- dplyr::group_by(ok_cooking_events,hh_id,stove,filename,month_year) %>% 
        dplyr::mutate(month_start = monthStart(start_time)) %>% #used to get timing window to figure out how many days to divide by to get the right uses/month calculation.
        dplyr::mutate(month_end = monthEnd(start_time)) %>%
        #For each hhid and stove, number of days logged for the given month
        dplyr::mutate(days_logged_per_hhid_stove_month = 
              case_when(start_date<=month_start & end_date>=month_start & end_date<=month_end ~ #1
                              as.numeric(difftime(end_date,month_start,units = "days")),
                        start_date<=month_start & end_date>=month_end ~  #2
                              as.numeric(difftime(month_end,month_start,units = "days")),
                        start_date>=month_start & end_date>=month_start & end_date<=month_end ~  #3
                              as.numeric(difftime(end_date,start_date,units = "days")),
                        start_date>=month_start & start_date<=month_end & end_date>=month_end ~  #4
                              as.numeric(difftime(month_end,start_date,units = "days")))) %>%
        dplyr::mutate(minutes_per_hhid_stove_month = sum(duration_minutes)) %>%
        dplyr::mutate(minutes_per_day_stove_hhid_month = minutes_per_hhid_stove_month/days_logged_per_hhid_stove_month) %>%
        dplyr::mutate(events_per_hhid_stove_month = n()) %>%
        dplyr::mutate(events_per_day_month = events_per_hhid_stove_month/days_logged_per_hhid_stove_month) %>%
        dplyr::filter(!is.na(stove_descriptions) | !is.na(start_date) | !is.na(end_date)) %>%
        dplyr::filter(days_logged_per_hhid_stove_month>1) %>%  #Need at least 1 day of data to present the data.
        dplyr::filter(!duplicated(hh_id) & !duplicated(stove) & !duplicated(month_year)) #Keep non-duplicated hh-by-month

             
#Stove-days time series. Number of households using a given stove type out of all houses monitoring that stove.
#Get number of hh's logging on the given day.  
#Get number of hh's cooking on the given day.
#Get rid of datest with 0 hh's logging.

events_summary_stovedays <- dplyr::group_by(ok_cooking_events,stove_descriptions,field_site,day_month_year) %>%
        #regroup to look for hh's contributing to the denominator.  Just need to check if the date of the cooking event is bounded by the start and end date of the other members of the group.
        dplyr::distinct(field_site,hh_id,stove_descriptions,day_month_year, .keep_all = TRUE) %>% #No duplicate events from the same hh contributing to stove-days.
        #Count number stove-days.  Must be calculated after removing multiple events per day for the same stove.
        dplyr::filter(duration_minutes >= cooking_duration_minimum) %>%
        dplyr::mutate(stove_days = n()) %>%  #Get counts for each group
        dplyr::distinct(field_site,stove_descriptions,day_month_year, .keep_all = TRUE)  #Get only one representative row from each stove day.

#Get nhouses_logging the old fashioned way of using a loop. 


```


# Usage rate code
* Plot usage rates (uses/day) by stove type, and region
```{r plot_stove_usage,echo=TRUE}

give.n1 <- function(x){
   return(c(y = -.3, label = length(x)))
}


give.n10 <- function(x){
   return(c(y = -25, label = length(x)))
}

#To make box and whiskers quantiles rather than IQRs.
f <- function(x) {
  r <- quantile(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}


#Plot average uses per day
gg <- ggplot(events_summary, aes(x=stove_descriptions, y = events_per_day)) + 
  stat_summary(fun.data = f, geom="boxplot") +    geom_jitter(width=0.3,alpha = 0.25) + 
  #facet_grid(stove_use_category~field_site,scales = "free", space = "free") + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Average uses/day",x="") + 
  stat_summary(fun.data = give.n1, geom = "text") + #Would give the number of households sampled.
#  stat_summary(fun.data = give.sum, geom = "text") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        text = element_text(size=14), plot.margin = margin(1,1,1,1, "cm"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))
gg
ggsave(filename="figures/AverageUsesPerDay.png", plot=gg,width = 10, height = 5)


#Plot average time used per day
gg1 <- ggplot(events_summary, aes(x=stove_descriptions, y = minutes_per_day_stove_hhid)) + 
  stat_summary(fun.data = f, geom="boxplot") +  
  geom_jitter(width=0.3,alpha = 0.25) + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Average minutes used/day",x="") + 
  stat_summary(fun.data = give.n10, geom = "text") + #Would give the number of households sampled.
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        text = element_text(size=14), plot.margin = margin(1,1,1,1, "cm"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))
gg1
ggsave(filename="figures/AverageTimeUsedPerDay.png", plot=gg1,width = 10, height = 5)


#Plot minutes by day (each data point shows 24h of data). For this plot, the daily data is shown, including days with no cooking.  Gives more balanced data points shown on the to long files with few houses compared with many short duration files.
daily_usage_time <-  dplyr::group_by(ok_cooking_events_padded,day_month_year,stove_descriptions,hh_id,field_site) %>%
  dplyr::summarise(daily_usage_time_by_stove = sum(duration_minutes,na.rm=TRUE))
  
gg1_24hdata <- ggplot(daily_usage_time, aes(x=stove_descriptions, y = daily_usage_time_by_stove)) + 
  stat_summary(fun.data = f, geom="boxplot") +  
  geom_jitter(width=0.3,alpha = 0.25) + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Minutes used per day",x="") + 
  stat_summary(fun.data = give.n10, geom = "text") + #Would give the number of households sampled.
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        text = element_text(size=14), plot.margin = margin(1,1,1,1, "cm"))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30))
gg1_24hdata
ggsave(filename="figures/TimeUsedPerDay_by_stove_by_HHID.png", plot=gg1_24hdata,width = 10, height = 5)



#Plot event duration per day by day of week
plotDayofWeekFunc <- function(x,y, na.rm = TRUE, ...) {
    gg1 <- ggplot(dplyr::filter(x,grepl(y,field_site,ignore.case=TRUE)), aes(x=day_of_week, y = duration_minutes)) + 
      stat_summary(fun.data = f, geom="boxplot") +    geom_jitter(width=0.1,alpha = 0.25) + 
      facet_wrap(stove_descriptions~field_site,scales = "free") + 
      labs(y="Cooking event duration (minutes)",x="") + 
      theme_bw()+
      theme(axis.text.x = element_text(angle = 60, hjust = 1))
      ggsave(gg1,filename=paste("figures/DurationbyDayofWeek",y,".png",sep=""))
}
 
plotDayofWeekFunc(ok_cooking_events,"China")
plotDayofWeekFunc(ok_cooking_events,"India")
plotDayofWeekFunc(ok_cooking_events,"Honduras")
plotDayofWeekFunc(ok_cooking_events,"Uganda")


#Plot average days sampled per file
gg2 <- ggplot(events_summary, aes(x=stove_descriptions, y = logging_duration_days)) + 
  stat_summary(fun.data = f, geom="boxplot") +   geom_jitter(width=0.1,alpha = 0.25) + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Days logged per file",x="") + 
  theme_bw()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        text = element_text(size=16))
gg2
ggsave(filename="figures/DaysSampledbyGroup.pdf", plot=gg2)


#Plot usage box plots by stove type and region
plotMonthlyFunc <- function(x,y, na.rm = TRUE, ...) {
    monthlyboxplots <- ggplot(dplyr::filter(x,grepl(y,field_site,ignore.case=TRUE)), aes(x=month_year, y = events_per_day_month,color = stove_descriptions)) + 
      stat_summary(fun.data = f, geom="boxplot",position = position_dodge(width=0.75)) +
      geom_jitter(width=0.2,alpha = 0.25) + 
      #geom_point(position=position_jitterdodge(dodge.width=0.75),alpha = 0.3) +
      labs(y="Mean events/day",x="") + 
      ggtitle(paste(y)) + 
      theme_bw()+
      theme(axis.text.x = element_text(angle = 60, hjust = 1),
            legend.title = element_blank(),
            text = element_text(size=16))
      monthlyboxplots
          ggsave(monthlyboxplots,filename=paste("figures/monthlyboxplots",y,".png",sep=""),
                  width = 7, height = 2)
}
 
plotMonthlyFunc(events_summary_monthly,"China")
plotMonthlyFunc(events_summary_monthly,"India")
plotMonthlyFunc(events_summary_monthly,"Honduras")
plotMonthlyFunc(events_summary_monthly,"Uganda")


#Plot stove-days
plotStoveDaysFunc <- function(x,y, na.rm = TRUE, ...) {
    plots <- ggplot(dplyr::filter(x,grepl(y,field_site,ignore.case=TRUE)), 
        aes(x=day_month_year, y = percent_stove_days,color = stove_descriptions)) + 
    geom_jitter(width=0.2,alpha = 0.25)   + geom_smooth(method='lm',formula=y~x) +
    ggtitle(paste(y,"stove-days")) + 
    facet_grid(~stove_use_category,scales = "free", space = "free") + 
    labs(y="% stove days",x="") + 
    theme_bw()+
    theme(legend.title = element_blank()) + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1),
          text = element_text(size=16))
    ggsave(plots,filename=paste("figures/PercentStoveDaysByGroup_",y,".png",sep=""))
    plots
}
 
#plotStoveDaysFunc(events_summary_stovedays,"China")
# plotStoveDaysFunc(events_summary_stovedays,"India")
# plotStoveDaysFunc(events_summary_stovedays,"Honduras")
# plotStoveDaysFunc(events_summary_stovedays,"Uganda")


#Time of day trend.  Remove groups with less than 5 hh's.
plotDensFunc <- function(x,y, na.rm = TRUE, ...) {
    plots <- ggplot(x, aes(start_hour, fill = stove_descriptions, 
                          colour = stove_descriptions)) + geom_density(alpha = 0.1) +
          labs(y="Time of day use density",x="Hour of day") +
          ggtitle(paste(y,"stove use density")) + 
          theme(plot.title = element_text(hjust = 0.5),
                text = element_text(size=16)) +
          theme_bw()+
          ylim(0, 0.15)
    plots
    ggsave(plots,filename=paste("figures/Density",y,".png",sep=""))
}

plotDensFunc(dplyr::filter(ok_cooking_events,!grepl("Uganda|India|honduras",filename)),"China")
plotDensFunc(dplyr::filter(ok_cooking_events,grepl("India",filename)),"India")
plotDensFunc(dplyr::filter(ok_cooking_events,grepl("Uganda",filename)),"Uganda")
plotDensFunc(dplyr::filter(ok_cooking_events,grepl("honduras",filename)),"Honduras")
```

# Honduras behavior data prep
* Compare usage with Honduras survey data

```{r honduras_behavior_comparison,echo=TRUE}
  #honduras_behavior vs. Usage in the time while the monitors were worn (time preceding the survey).  Can compare behaviortimes_cook ("Since you've been wearing the monitors, how many times did you cook a meal or make coffee? "), behaviorsec_cook ("Since you've been wearing the monitors, how many times did you cook a meal or make coffee with the secondary?"), behaviorhours_primary ("How many hours per day do you usually cook on your primary stove?"), and behaviorhours_secondary ("If you have a secondary stove, how many hours per day do you usually cook on your secondary stove?")
  #honduras_behavior data has the date set at midnight UTC, so it looks like the survey date is coarse, so I will look back 48 hours initially to ensure that I get the previous deployment time of usage, and forward 48h for the same reason.  There is no risk here as the data files were already well organized and would not have long durations with a mislabeled file name.
honduras_behavior <- dplyr::mutate(honduras_behavior,hh_id = house_id) 

ok_cooking_events_honduras <-dplyr::filter(ok_cooking_events,!grepl("PAZ",hh_id,ignore.case=TRUE))  %>%  #Removing the PAZ households, which used thermocouples and had no comparable survey data.
 dplyr::filter(grepl("honduras",field_site,ignore.case=TRUE))


honduras_reshaped <- dplyr::select(honduras_behavior,hh_id,behaviorhours_primary,behaviorhours_secondary) %>%
            tidyr::gather(key = category,value = survey_hours, behaviorhours_primary,behaviorhours_secondary ) %>%
            dplyr::mutate(hhid_category = if_else(grepl("primary",category),paste0(hh_id,"Primary stove"),paste0(hh_id,"Secondary stove")))

honduras_behavior_comparison_time <- dplyr::filter(ok_cooking_events_honduras,grepl("honduras",field_site,ignore.case=TRUE)) %>%
          dplyr::group_by(hh_id,stove_use_category) %>%
          dplyr::mutate(sums_cooking_time = sum(duration_minutes)/60/logging_duration_days) %>% #Get ave cooking time in hours
          dplyr::ungroup()%>%
          dplyr::mutate(hhid_category = if_else(grepl("primary",stove_use_category,ignore.case=TRUE),paste0(hh_id,"Primary stove"),
              if_else(grepl("secondary",stove_use_category,ignore.case=TRUE),paste0(hh_id,"Secondary stove"),
                      paste0(hh_id,"tertiary"))))  %>%
          dplyr::mutate(stove_use_category = if_else(grepl("Primary",stove_use_category,ignore.case=TRUE),"Primary stove",
              if_else(grepl("secondary",stove_use_category,ignore.case=TRUE),"Secondary stove",
                      "Tertiary stove")))  %>%
          dplyr::left_join(honduras_reshaped,'hhid_category') %>%
          dplyr::select(hhid_category,stove_use_category,sums_cooking_time,survey_hours) %>%
          dplyr::rename('SUM cooking time' = sums_cooking_time,'Survey time'  = survey_hours) %>%
          dplyr::distinct(hhid_category, category, .keep_all = TRUE) %>%
          tidyr::gather(key = category,value =hours, 'SUM cooking time' ,'Survey time')
honduras_behavior_comparison_events_paired <-  dplyr::group_by(ok_cooking_events_honduras,hh_id,stove_use_category) %>%
          dplyr::mutate(usage_events = n()) %>%
          dplyr::left_join(honduras_behavior,'hh_id') %>%
          dplyr::mutate(survey_events = behaviortimes_cook) %>%
          dplyr::distinct(hh_id,.keep_all = TRUE)
        
          
#Get event info, so we can compare the number of events in a given household from the monitoring period according to surveys, with those from SUMs
#SUMs events are summed between primary and secondary, as the question was general and not specific to the stove:
#"Since you've been wearing the monitors, how many times did you cook a meal or make coffee?"
honduras_behavior_comparison_events <-  dplyr::group_by(ok_cooking_events_honduras,hh_id) %>%
          dplyr::mutate(usage_events = n()) %>%
          dplyr::left_join(honduras_behavior,'hh_id') %>%
          dplyr::mutate(survey_events = behaviortimes_cook) %>%
          dplyr::distinct(hh_id,.keep_all = TRUE)
        
```

# Plot time in sums vs. survey time used regularly
* Compare usage with Honduras survey data (violin)

```{r plot_violin_behavior_comparison2,echo=TRUE}

#TIME
#Participants were asked during the survey ‘how many hours per day do you usually cook on your primary/secondary stove(s)?’  These self-reported data can be compared with the number of hours cooked as measured with SUMs
#Violin plot
gg_hours_violin <- ggplot(honduras_behavior_comparison_time, aes(x=category, y = hours)) + 
  # stat_summary(fun.data = f, geom="boxplot",width = 0.5) + 
  geom_violin() +
  geom_boxplot(width = 0.3) +
  geom_jitter(width=0.15,alpha = 0.25) + 
  labs(y="Daily average cooking time (hours/day)") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  facet_grid(~stove_use_category,scales = "free", space = "free") + 
  ggtitle("Measured stove use and reported normal stove usage") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  stat_summary(fun.y=mean, colour="red", geom="point",group=1,shape = 8,alpha = 0.5,size = 4)+
  theme_bw()
  #xlim(-0, 16) +  ylim(-0, 16) 
print(gg_hours_violin)
ggsave(filename="figures/Honduras_survey_hours_a48_viol.pdf", plot=gg_hours_violin,width = 10, height = 5)

```

# Plot spearmans time in sums vs. survey time used regularly
* Compare usage with Honduras survey data (spearman)

```{r plot_scatter_behavior_comparison2,echo=TRUE}

data_wide_honduras <- spread(honduras_behavior_comparison_time, category, hours) %>%
          tidyr::drop_na()

primary_hours <-dplyr::filter(data_wide_honduras,stove_use_category=="Primary stove") %>%
  dplyr::filter(!is.na(hours)) %>% dplyr::filter(!is.na(stove_use_category))

#Use this in paper. Primary stove
spearmantest_primary <- cor.test(y=primary_hours$`SUM cooking time`,x=primary_hours$`Survey time`, data = primary_hours,method = "spearman")
print(spearmantest_primary)
sums_survey_primaryhours_regression <- lm(`SUM cooking time` ~ `Survey time`,data=primary_hours)#Pearson
summary(sums_survey_primaryhours_regression)

#Scatter plot (primary only)
gghours_scatter <- ggplot(primary_hours, aes(x=`SUM cooking time`,y=`Survey time`)) + 
  labs(y="Typical reported usage time with primary stove" ,x="SUM-measured usage time during monitoring period") + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_smooth(method='lm',formula=y~x) + 
  geom_abline(slope=1, intercept=0,linetype="dashed") + 
  ggtitle("Typical reported usage time with primary stove vs.SUM-measured usage time during monitoring period") +
  theme_bw() +
  xlim(0, 12) +  ylim(0, 12) 
gghours_scatter <- ggMarginal(gghours_scatter, type = "histogram",binwidth = 1)
print(gghours_scatter)
ggsave(filename="figures/Honduras_survey_hours_scatter_primary.pdf", plot=gghours_scatter,width = 6, height = 6)

secondary_hours <-dplyr::filter(data_wide_honduras,stove_use_category=="Secondary stove") %>%
  dplyr::filter(!is.na(hours)) %>% dplyr::filter(!is.na(stove_use_category))

#Use this in paper. Secondary stove
spearmantest_secondary <- cor.test(y=secondary_hours$`SUM cooking time`,x=secondary_hours$`Survey time`, data = secondary_hours,method = "spearman")
print(spearmantest_secondary)
sums_survey_secondary_hours_regression <- lm(`SUM cooking time` ~ `Survey time`,data=secondary_hours)#Pearson
summary(sums_survey_secondary_hours_regression)
```

# Plot ROBUST spearmans time in sums vs. survey time used regularly
* Compare usage with Honduras survey data (spearman)

```{r plot_scatterrobust_behavior_comparison2,echo=TRUE}
#Robustness. Remove 10% of outliers and try it out to check for robustness. "with no significant difference (p=0.85) after removing the highest 10% from the SUMs and survey data."
#Primary
hours_primary_robust <- dplyr::filter(primary_hours,quantile(`SUM cooking time`, 0.9)>`SUM cooking time`) %>%
          dplyr::filter(quantile(`Survey time`, 0.9)>`Survey time`)
spearmantest_primary_robust <- cor.test(y=hours_primary_robust$`SUM cooking time`,x=hours_primary_robust$`Survey time`, data = hours_primary_robust,method = "spearman")
print(spearmantest_primary_robust)

#Scatter plot (primary robust only)
gghours_scatter_robust <- ggplot(hours_primary_robust, aes(x=`SUM cooking time`,y=`Survey time`)) + 
  labs(y="Typical reported usage time from primary stove" ,x="SUM-measured usage time during monitoring period") + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_smooth(method='lm',formula=y~x) + 
  geom_abline(slope=1, intercept=0,linetype="dashed") + 
  ggtitle("Typical reported usage time with robust set primary stove vs.SUM-measured usage time during monitoring period") +
  theme_bw() +
  xlim(0, 12) +  ylim(0, 12) 
gghours_scatter_robust <- ggMarginal(gghours_scatter_robust, type = "histogram",binwidth = 1)
print(gghours_scatter_robust)
ggsave(filename="figures/Honduras_survey_hours_scatter_primaryrobust.pdf", plot=gghours_scatter_robust,width = 6, height = 6)

#Secondary
hours_secondary_robust <- dplyr::filter(secondary_hours,quantile(`SUM cooking time`, 0.9)>`SUM cooking time`) %>%
          dplyr::filter(quantile(`Survey time`, 0.9)>`Survey time`)
spearmantest_secondary_robust <- cor.test(y=hours_secondary_robust$`SUM cooking time`,x=hours_secondary_robust$`Survey time`, data = hours_secondary_robust,method = "spearman")
print(spearmantest_secondary_robust)


# event_kw_secondary <- kruskal.test(as.vector(secondary_hours$hours),as.factor(secondary_hours$category))
# print(event_kw_secondary)
# event_ttest_secondary <- t.test(hours~category,data=secondary_hours)
# print(event_ttest_secondary)
# summary_stats_secondary_hours <- dplyr::group_by(secondary_hours,category) %>%
#       dplyr::summarise(mean_hours=mean(hours, na.rm = TRUE),
#                   median_hours=median(hours, na.rm = TRUE))
# kable(summary_stats_secondary_hours)

```


# Plot the Honduras survey vs. SUMs results

```{r plot_honduras_relationships,echo=TRUE}

#EVENTS
#Survey event comparison with total sums from the household. The first relevant survey question (questions are listed in SI Table S2) asked the participant ‘since you’ve been wearing the monitor, how many times did you cook a meal or make coffee?’  Since the SUMs monitoring coincided with personal exposure monitoring, these survey data provide a point of comparison with the number of usage events measured with SUMs, with the caveat that the survey asks specifically about the participant cooking and making coffee, and not the total household stove usage as SUMs would inherently measure.
gd <- honduras_behavior_comparison_events %>% group_by(field_site) %>%
        summarise(
          usage_events_mean = mean(usage_events,na.rm = TRUE),
          survey_events_mean = mean(survey_events,na.rm = TRUE),
          usage_events_med = median(usage_events,na.rm = TRUE),
          survey_events_med = median(survey_events,na.rm = TRUE))
gg4 <- ggplot(honduras_behavior_comparison_events, aes(x=usage_events, y = survey_events)) + 
  geom_jitter(width=0.2,height=0.1,alpha = 0.25) + 
  labs(y="Recalled cooking/coffee-making events during monitoring period"
       ,x="SUM-measured usage events during monitoring period") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_smooth(method='lm',formula=y~x) + 
  geom_abline(slope=1, intercept=0,linetype="dashed") + 
  ggtitle("Honduras reported vs. measured stove use for all stoves in household") +
  theme_bw() +
  # geom_point(data = gd, aes(x=usage_events_mean, y = survey_events_mean), colour="red", geom="point",shape = 18,alpha = 0.5,size = 5.5)+
  geom_point(data = gd, aes(x=usage_events_med, y = survey_events_med), colour="red", geom="point",shape = 8,alpha = 0.5,size = 5.5)+#Get overall averages
  xlim(0, 12) +  ylim(0, 12) 
gg4 <- ggMarginal(gg4, type = "histogram",binwidth = 1)
gg4
ggsave(filename="figures/Honduras_survey_events_b48.pdf", plot=gg4,width = 6, height = 6)



sums_survey_regression <- lm(survey_events ~ usage_events, data=honduras_behavior_comparison_events)#Pearson
summary(sums_survey_regression)

summary_stats_honduras_comparison <- dplyr::ungroup(honduras_behavior_comparison_events) %>%
      dplyr::summarise(mean_recalled_events=mean(survey_events, na.rm = TRUE),
                  median_recalled_events=median(survey_events, na.rm = TRUE),
                  sd_recalled_events=sd(survey_events, na.rm = TRUE),
                  mean_SUMs_events=mean(usage_events, na.rm = TRUE),
                  median_SUMs_events=median(usage_events, na.rm = TRUE),
                  sd_SUMs_events=sd(usage_events, na.rm = TRUE))

kable(summary_stats_honduras_comparison)                       
                       

#Bland-Altman plot:
honduras_behavior_comparison_events <- honduras_behavior_comparison_events[!is.na(honduras_behavior_comparison_events$survey_events), ]
honduras_behavior_comparison_events$Avg <- (honduras_behavior_comparison_events$usage_events + honduras_behavior_comparison_events$survey_events) / 2
honduras_behavior_comparison_events$Dif <- honduras_behavior_comparison_events$usage_events - honduras_behavior_comparison_events$survey_events
# Finally, code the plot and add the mean difference (blue line) and a 95% confidence interval (red lines) for predictions of a mean difference. This prediction interval gives the level of agreement (1.96 * SD).

ggbland <- ggplot(honduras_behavior_comparison_events, aes(x = Avg, y = Dif)) +
  geom_jitter(width=0.1,height=0.1,alpha = 0.25) + 
  geom_hline(yintercept = mean(honduras_behavior_comparison_events$Dif), colour = "blue", size = 0.5) +
  geom_hline(yintercept = mean(honduras_behavior_comparison_events$Dif) - (1.96 * sd(honduras_behavior_comparison_events$Dif)), colour = "red", size = 0.5) +
  geom_hline(yintercept = mean(honduras_behavior_comparison_events$Dif) + (1.96 * sd(honduras_behavior_comparison_events$Dif)), colour = "red", size = 0.5) +
  ylab("Diff. between measures (events per day)") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  ggtitle("Bland-Altman plot for SUMs vs. survey cooking events") +
  theme_bw() + #  xlim(0, 12) + ylim(-12, 12) +
  xlab("Average measure (events per day)")
ggsave(filename="figures/Bland-Altman_cooking_events.pdf", plot=ggbland,width = 6, height = 6)


# gg3 <- ggplot(honduras_behavior_comparison_time, aes(x=hours, y = survey_hours,color = stove_descriptions)) + 
#   geom_jitter(alpha = 0.25) + 
#   labs(y="Reported average cooking time (hours)",x="Cooking time during monitoring period (hours)") +
#   theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
#   geom_smooth(method='rlm') + 
#   geom_abline(slope=1, intercept=0,linetype="dashed") + 
#   ggtitle("Honduras reported vs. measured stove use") +
#   theme(plot.title = element_text(hjust = 0.5)) + 
#   theme_bw()+
#   xlim(-0, 13) +  ylim(-0, 13) + 
#   geom_text(aes(label=ifelse(survey_hours>7.5 | survey_hours<1.5,substr(stove_use_category,1,1),'')),
#             angle = 60,hjust=0,vjust=0,size=3)
# gg3
# ggsave(filename="figures/Honduras_survey_hours_c48.pdf", plot=gg3,width = 10, height = 5)
# 
# gg4 <- ggplot(events_honduras_behavior, aes(x=usage_events, y = survey_events,color = stove_descriptions)) + 
#   geom_jitter(width=0.1,height=0.1,alpha = 0.25) + 
#   labs(y="Recalled cooking events during monitoring period",x="SUM-measured cooking events during monitoring period") + 
#   theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
#   theme(plot.title = element_text(hjust = 0.5)) + 
#    geom_smooth(method = "glm", 
#        method.args = list(family = "poisson"), linetype = "dashed")+
#   #geom_smooth(method='rlm',formula=y~x) + 
#   geom_abline(slope=1, intercept=0,linetype="dashed") + 
#   ggtitle("Honduras reported vs. measured stove use") +
#   xlim(-0, 13) +  ylim(-0, 13) +   
#   theme_bw()+
#   geom_text(aes(label=ifelse(survey_hours>7.5 | survey_hours<1.5,substr(stove_use_category,1,1),'')),
#             angle = 60,hjust=0,vjust=0,size=3)
# gg4
# 
# ggsave(filename="figures/Honduras_survey_hours_d48.pdf", plot=gg4,width = 10, height = 5)

```


# Summary

Temperature was measured for `r length(unique(field_sumsarized_events_all$hh_id))` experiments between `r min(field_sumsarized_events_all$date, na.rm = TRUE)` and `r max(field_sumsarized_events_all$date, na.rm = TRUE)`. There is no temperature data for tests: `r setdiff(as.character(field_samples$hh_id), as.character(field_sumsarized_events_all$hh_id))`.

Temperature data is expected to be missing for: no tests.

## Save files

* save data

```{r save_data}
  saveRDS(ok_cooking_events, file = "../r_files/field_ok_cooking_events.RDS")
  saveRDS(field_sumsarized_timeseries_all, file = "../r_files/field_sumsarized_timeseries_all.RDS")

```


