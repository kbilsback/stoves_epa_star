---
title: "field_sumsarized"
author: "ricardo_piedrahita"
date: "8/10/2017"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r library, message=FALSE, warning=FALSE, echo=FALSE}
  library(tidyverse)
  library(knitr)
  library(lubridate)
  library(dplyr)
  library(plyr)
  library(magrittr)
  library(data.table)
  library(ggplot2)
  library(ggpubr)
  library(plotly)
```

```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.path='figures/', warning=FALSE, message=FALSE, cache=FALSE)
```

```{r functions}
  source("../r_scripts/functions.R")
  source("../r_scripts/plots.R")
  source("../r_scripts/filter_sum_data.R")
```

# Load data

* Time stamps are fixed in the data importing step

```{r load_data}
  field_sumsarized_events <- readRDS("../r_files/field_sumsarized_events.RDS") #Data analyzed by Geocene, directly giving events
  field_sumsarized_csvdurations <- readRDS("../r_files/field_sumsarized_durations.RDS") #CSVs used by Geocene to get events.  These are needed so that we know the sampling time duration for average usage stats.
  field_sumsarized_timeseries <- readRDS("../r_files/field_sumsarized_timeseries.RDS") #Import data that was analyzed with the web-based sumsarizer tool.
```

* metadata

```{r load_meta}
  field_test_times <- readRDS("../r_files/field_test_times.RDS")
  field_samples <- readRDS("../r_files/field_samples.RDS")
  field_temp_meta <- readRDS("../r_files/field_temp_meta.RDS")
  field_notes <- readRDS("../r_files/field_notes.RDS")
  honduras_behavior <-readRDS("../r_files/honduras_behavior.RDS") #Import self-reported data from Honduras.
```

* Define variables for coding stoves and for filtering out data.  Also set some constants for the stove event analysis

```{r define_variables}
  stove_codes <- data.frame(stove = as.factor(c("Metal jiko","Traditional ceramic","Metal jiko ceramic","Uga",
                  "Traditional ceramic metal clad","Kerosene","Tire Rim",
                  "Traditional chimney","KNG","CLH","CLC","IBP","CLH1","Other",
                  "PB-OK","PB-IK", "PB-IK-E","LPG","TSF","PB-OK Biomass mobile",
                  "RS-IK","chimney","traditional","chimney2")),
            stove_descriptions = as.factor(c("Jiko-style", "Jiko-style", "Jiko-style",
                  "Jiko-style","Jiko-style","Kerosene", "Tire rim","Traditional chimney",
                  "Kang","Coal heating with chimney","Coal cooking with chimney",
                  "Biomass pellet with chimney", "Coal heating with chimney","Other",
                  "Chulha", "Chulha", "Chulha", "LPG", "Three stone fire", "Ceramic rocket", 
                  "Rocket","Chimney","Three stone fire","Chimney")))
  
  cooking_group <- 10 # x minute window for grouping events together.
  cooking_duration_minimum <- 10
  
  
  # Remove data from files selected as bad (they match the following strings: 0_0_3_P54637_IndiaDMYminave
  bad_files <- paste(c("0_0_3_P83601_IndiaDMYminave","|0_0_3_P46673_Indiaminaves","|0_0_3_P54588_Indiaminaves"
                 ,"|0_0_3_P46673_Indiaminaves","|0_0_0_P57934_Indiaminaves","|0_0_0_P83591_IndiaDMYminaves"
                 ,"|0_0_0_P83601_Indiaminaves","|0_0_2_P57934_Indiaminaves","|0_0_2_P83591_Indiaminaves"
                 ,"|0_0_3_P54637_IndiaDMYminaves","|0_0_4_P46673_IndiaDMYminaves","|IN7_P54593_20160419_Indiaminaves"
                 ,"|IN8_P83591_20160419_Indiaminaves","|IN8_P83591_20160503_IndiaDMYMinAves",
                 "|IN10_P57934_20160427_Indiaminaves"
                 ,"|IN10_P57934_20160503_Indiaminaves","|IN11_P46673_20160420_Indiaminaves",
                 "|IN11_P46673_20160503_IndiaDMYMinAves"
                 ,"|IN12_P54637_20160503_IndiaDMYMinAves","|INXX_P53257_20160430_Indiaminaves",
                 "|INXX|0_0_0_P53257_IndiaMinAves"),collapse="")

```

# Tidy

* add household id information, get metadata from metadata file and from file names, depending on country
```{r parse_add_metadata}

#Use the same filtering on both the data set prepared by Geocene that gave events, and the data output by Sumsarizer that gives a time series.
field_sumsarized_events_all<-filter_sum_data(field_sumsarized_events,field_temp_meta)

field_sumsarized_events_all <-  dplyr::left_join(field_sumsarized_events_all,
                                              dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                              dplyr::mutate(start_time = if_else(grepl("add 11.5",notes),
                              start_time+hours(11) + minutes(30), start_time)) %>% #If datapoint has a comment with the text 11.5, add that to the timestamp.
                              dplyr::group_by(event_num) %>% #There are some duplicate cooking event numbers because during the join between the meta data and the data set, it was not clear which sum was used to generate the cooking event...but it does not matter, because the stove and hhid were the same.  This was the case for Uganda data that had the hhid in the filename and was used for matching rather than the sum id.
                              dplyr::filter(row_number() == 1) %>%
                              dplyr::ungroup() %>% ##If data has an end time, keep only the data between the start and end time
                              dplyr::filter(((start_time<end_date & start_time>start_date) | is.na(end_date))) %>%
                              dplyr::mutate(units = "degrees Celsius") %>%#Define units as degrees C
                              dplyr::filter(duration_minutes>cooking_duration_minimum) %>%
                              dplyr::group_by(filename) %>%
                              dplyr::mutate(events_per_file = n()) %>%
                              dplyr::mutate(events_per_day = events_per_file/logging_duration_days) %>%
                              dplyr::select(filename,start_time,duration_minutes,hh_id,stove,logger_id,field_site,
                                stove_use_category,stove_descriptions,stove,logging_duration_days,units,events_per_day)


  
#Import csv files of all the event data to get the logging duration from each file.
field_sumsarized_events_all_durations<-filter_sum_data(field_sumsarized_csvdurations,field_temp_meta)


#Import thermocouple data time series output from Sumsarizer.
field_sumsarized_timeseries_all <-filter_sum_data(field_sumsarized_timeseries,field_temp_meta)
field_sumsarized_timeseries_all <- dplyr::mutate(field_sumsarized_timeseries_all,
                          datetime = if_else(grepl("add 11.5",notes),
                          datetime+hours(11) + minutes(30), datetime)) %>% #If datapoint has a comment with 11.5, add time
                          dplyr::filter(((datetime<end_date & datetime>start_date) | is.na(end_date)))


```

  
* Import thermocouple sumsarizer data
* Form cooking events from raw sumsarizer output don't need it for the event data generated by Danny Wilson
* Group distinct cooking events together.  If they are within cooking_group minutes of each other.
* Only need this for data that went through the web-based sumsarizer, where events are not made automatically.

```{r group_cooking_events}

# # start counting up for cooking events.  Add columns for start and stop times of cooking events, and keep the hhid, loggerid, stove type, field site.  Keep only cooking event time points.  Need a case for start (if i = 1), if data point is within 30minutes of the last one, if data point is greater than 30 minutes from the last one.

# #Initialize the cooking_events frame.
 cooking_events_timeseries <- data.frame(start_time=as.POSIXct(character()),end_time=as.POSIXct(character()), field_site=factor(),  hh_id=factor(), logger_id=factor(),stove=factor(), logging_start_time=as.POSIXct(character()), logging_end_time=as.POSIXct(character()),file_indices = as.numeric(),events_per_file = as.numeric(),stove_use_category=factor())

 #for each SUM data file
 for (i in unique(field_sumsarized_timeseries_all$file_indices)) {
   #Grab data from file i, and keep only the entries that are marked as cooking
   temp <- dplyr::filter(field_sumsarized_timeseries_all,file_indices == i) %>%
     dplyr::filter(state == TRUE)

   difftime <- as.numeric(diff(temp$datetime)) #Time between identified cooking samples, in minutes
   breakstart <- c(0,which((difftime>cooking_group) == TRUE))+1 #Start of a cooking event
   breakend <- c(which((difftime>cooking_group) == TRUE),if (tail(temp$state,n=1) == TRUE){dim(temp)[1]}) #End of cooking event. Last part is in case the time series ends while still cooking...need to account for that.
   #Add cooking events to the cooking_events_timeseries data frame.
   cooking_events_timeseries <- rbind(cooking_events_timeseries,data.frame(start_time=as.POSIXct(temp$datetime[breakstart]),end_time=as.POSIXct(temp$datetime[breakend]), field_site=as.factor(temp$field_site[breakstart]), hh_id=as.factor(temp$hh_id[breakstart]), logger_id=factor(temp$logger_id[breakstart]),stove=factor(temp$stove[breakstart]), logging_start_time=as.POSIXct(temp$datetime[1]),logging_end_time = as.POSIXct(max(temp$datetime)),file_indices = temp$file_indices[breakstart], events_per_file = length(breakstart)*breakstart/breakstart,filename = temp$filename[breakstart],
stove_use_category=factor(temp$stove_use_category[breakstart])))

 }

#Clean up cooking events.  Removing events with a single sample that indicates cooking, more than 30 minutes from other events.
 cooking_events_timeseries <- dplyr::left_join(cooking_events_timeseries,
                                              dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                              dplyr::mutate(units = "degrees Celsius") %>% #Define units as degrees C
                              dplyr::mutate(duration_minutes = as.numeric(difftime(end_time,start_time,units = "mins"))) %>%
                              dplyr::mutate(logging_duration_days = as.numeric(difftime(logging_end_time,
                                              logging_start_time,units = "days"))) %>%
                              dplyr::filter(duration_minutes>cooking_duration_minimum) %>%
                              dplyr::mutate(events_per_day = events_per_file/logging_duration_days) %>%
                              dplyr::mutate(hour_of_day = ((hour(end_time) + minute(end_time)/60) + (hour(start_time) +
                                              minute(start_time)/60))/2) %>%
                              dplyr::select(filename,start_time,duration_minutes,hh_id,stove,logger_id,field_site,
                                              stove_use_category,stove_descriptions,stove,logging_duration_days,
                                              units,events_per_day)

 #Data summaries
 # simpledatasummary <- summarise_each(field_sumsarized_events_all,funs(n_distinct(.)))
 # 
 # grouped_summary <- cooking_events_timeseries %>% dplyr::group_by(field_site,stove_descriptions) %>%
 #    dplyr::summarize(mean=mean(duration_minutes, na.rm = TRUE), median=median(duration_minutes, na.rm = TRUE),sd=sd(duration_minutes, na.rm = TRUE),n=n())
 # 
 # #Get a single row from each file so we can look at the stats calculated previously.
 #  hh_file_average_summary <- dplyr::filter(cooking_events_timeseries,!duplicated(filename))
```

* Create and save final cooking event data frame.

```{r clean_cooking_events}

#Combine cooking events from the SUMsarized data set by Danny, with the ones from the thermocouples.
  all_cooking_events <- plyr::rbind.fill(field_sumsarized_events_all,cooking_events_timeseries)
  saveRDS(all_cooking_events, file = "../r_files/all_cooking_events.RDS")

```


# Remove data from bad files:
  * merge flags with data
  * Need to test this
```{r create_flags}
  # fix warning
  all_cooking_events <-  dplyr::mutate(all_cooking_events,
                                       qc = if_else(grepl(bad_files,filename,ignore.case=TRUE),"bad","ok")) %>% 
                                       dplyr::filter(!grepl("ok",qc,ignore.case=TRUE))

  
```
#Remove data from SUMs that were secondary measurements (this happened in Honduras during piloting).  To do this, remove data from chimney2 - these were duplicate placements.

#Clean up the event data, define units, set up some grouping variables
all_cooking_events <- 
        dplyr::group_by(all_cooking_events,hh_id,stove) %>%   #group_by is handy.
        dplyr::mutate(events_per_hhid_stove = n()) %>%
        dplyr::mutate(minutes_per_hhid_stove = sum(duration_minutes)) %>%
        dplyr::arrange(hh_id) 

#Summarize use by household/deployment/stove for plotting average use.
events_summary <- dplyr::filter(all_cooking_events,!duplicated(filename)) %>%
        dplyr::group_by(hh_id,stove) %>%   
        dplyr::mutate(days_logged_per_hhid_stove = sum(logging_duration_days))   %>%
        dplyr::mutate(events_per_day = events_per_hhid_stove/days_logged_per_hhid_stove) %>%
        dplyr::mutate(minutes_per_day_stove_hhid = minutes_per_hhid_stove/days_logged_per_hhid_stove) %>%
        dplyr::distinct(filename, .keep_all = TRUE)

```


* set timezones based on field site

```{r fix_timezones}
  all_cooking_events <- dplyr::mutate(all_cooking_events,
                                         datetime = lubridate::force_tz(datetime,
                                                                        tzone = "Asia/Calcutta"))
```


* Plot usage rates (uses/day) by stove type, and region
```{r plot_stove_usage}

give.n <- function(x){
   return(c(y = 0, label = length(x)))
}

#To make box and whiskers quantiles rather than IQRs.
f <- function(x) {
  r <- quantile(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}

#Plot average uses per day
gg <- ggplot(events_summary, aes(x=stove_descriptions, y = events_per_day)) + 
  stat_summary(fun.data = f, geom="boxplot") + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Average uses/day",x="") + 
  stat_summary(fun.data = give.n, geom = "text") +
#  stat_summary(fun.data = give.sum, geom = "text") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
  # geom_text(data = events_summary %>% 
  #                        group_by(stove_descriptions) %>% 
  #                        summarise(ndays = list(sum(days_logged_per_hhid_stove))) %>% 
  #                        tidyr::unnest(), 
  #              aes(x = factor(stove_descriptions), y = -.5, label = paste("days=",ndays)), 
  #              nudge_x = .5)
gg
ggsave(filename="figures/AverageUsesPerDay.pdf", plot=gg)




#Plot average time used per day
gg1 <- ggplot(events_summary, aes(x=stove_descriptions, y = minutes_per_day_stove_hhid)) + 
  stat_summary(fun.data = f, geom="boxplot") + 
  facet_grid(~field_site,scales = "free", space = "free") + 
  labs(y="Average minutes used/day",x="") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
gg1
ggsave(filename="figures/AverageTimeUsedPerDay.pdf", plot=gg1)


plotDensFunc <- function(x,y, na.rm = TRUE, ...) {
    plots <- ggplot(x, aes(hour_of_day, fill = stove_descriptions, 
                          colour = stove_descriptions)) + geom_density(alpha = 0.1) +
          labs(y="Time of day use density",x="Hour of day") +
          ggtitle(paste(y,"stove use density")) + 
          theme(plot.title = element_text(hjust = 0.5)) +
          ylim(0, 0.15)
    ggsave(plots,filename=paste("figures/Density",y,".png",sep=""))
}
plotDensFunc(dplyr::filter(all_cooking_events,!grepl("Uganda|India|honduras",filename)),"China")
plotDensFunc(dplyr::filter(all_cooking_events,grepl("India",filename)),"India")
plotDensFunc(dplyr::filter(all_cooking_events,grepl("Uganda",filename)),"Uganda")
plotDensFunc(dplyr::filter(all_cooking_events,grepl("honduras",filename)),"Honduras")

```

* Compare usage with Honduras survey data

```{r honduras_behavior_comparison}
  #honduras_behavior vs. Usage in the 24h preceding the survey.  Can compare behaviortimes_cook (number of times the primary stove was used to cook), behaviorsec_cook (times the secondary stove was used to cook), behaviorhours_primary (hours cooked with primary stove), and behaviorhours_secondary (hours cooked with secondary stove)
  #honduras_behavior data has the date set at midnight UTC, so it looks like the survey date is coarse, so I will look back 36 hours initially to ensure that I get the previous 24h of usage, and forward 36h for the same reason.  There is no risk here as the data files were already well organized and would not have long durations with a mislabeled file name.
datalist = list()

 for (i in 1:dim(honduras_behavior)[1]) {
  temp <- dplyr::filter(all_cooking_events,grepl(honduras_behavior$house_id[i],hh_id)) %>%
          dplyr::filter(start_time<honduras_behavior$date[i]+3600*36 & start_time>honduras_behavior$date[i]-3600*36) %>%
          dplyr::group_by(stove_use_category) %>%
          dplyr::mutate(usage_events = n()) %>%
          dplyr::mutate(usage_hours = sum(duration_minutes)/60) %>%
          dplyr::filter(!duplicated(filename)) %>%
          dplyr::mutate(survey_events = if_else(grepl("Primary",stove_use_category),
                                                honduras_behavior$behaviortimes_cook[i],
                                                if_else(grepl("Secondary",stove_use_category)
                                                        ,honduras_behavior$behaviorsec_stove[i],999))) %>%
          dplyr::mutate(survey_hours = if_else(grepl("Primary",stove_use_category),
                                                honduras_behavior$behaviorhours_primary[i],
                                                if_else(grepl("Secondary",stove_use_category)
                                                        ,honduras_behavior$behaviorhours_secondary[i],999)))

    datalist[[i]] <- temp # add it to your list
}

honduras_behavior_compare = do.call(rbind, datalist)

```

# QC

## flags from notes

* extract notes

```{r get_notes}
  field_notes <- dplyr::filter(field_notes, grepl("sumsarized|all", field_notes$inst) == TRUE)
```




## additional bad tests

```{r bad_tests_2}
  all_cooking_events$qc[all_cooking_events$hh_id == "IN4"] <- "maybe"
```

## Filter only test

```{r get_times}
  field_times <- dplyr::filter(field_test_times, var == "sample_start" | var == "sample_end") %>%
                 tidyr::spread(var, value) %>%
                 dplyr::rename(start = sample_start, end = sample_end)
```

```{r filter_tests}
  # currently missing all data from emissions households
  #field_sumsarized_events <- filter_times(field_times, field_sumsarized_events_all)
```

# Plots

## all data

```{r plot_all_temp_data, fig.width=12, fig.height=20}
  field_timeseries_plot(field_sumsarized_events_all, "stove_temp")
```

## data summary

```{r boxplot_temp, fig.width=12, fig.height=4}
  field_boxplot(field_sumsarized_events_all, "stove_temp")
```

## test data only

```{r plot_test_temp, fig.width=12, fig.height=10}
  # missing all test day data
  #field_timeseries_plot(field_temp_merged, "temp")
```

# Summary

Temperature was measured for `r length(unique(field_sumsarized_events_all$hh_id))` experiments between `r min(field_sumsarized_events_all$date, na.rm = TRUE)` and `r max(field_sumsarized_events_all$date, na.rm = TRUE)`. There is no temperature data for tests: `r setdiff(as.character(field_samples$hh_id), as.character(field_sumsarized_events_all$hh_id))`.

Temperature data is expected to be missing for: no tests.

## Save files

* put data into long format

```{r long_data_conversion}
  field_sumsarized_events_all <- dplyr::select(field_sumsarized_events_all, field_site, hh_id, date, time, datetime, stove_temp, units, qc) %>%
                           tidyr::gather("var", "val", 6)

  #field_sumsarized_events <- dplyr::select(field_sumsarized_events, hh_id, date, time, datetime, stove_temp, units, qc, field_site) %>%
                       #tidyr::gather("var", "val", 5:8)
```

* save data

```{r save_data}
  saveRDS(field_sumsarized_events_all, file = "../r_files/field_sumsarized_events_all.RDS")
  #saveRDS(field_sumsarized_events, file = "../r_files/field_sumsarized_events.RDS")
```


